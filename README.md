<!-- Project Overview -->
# Fine-tuning BERT for Question Answering

## Overview
BERT (Bidirectional Encoder Representations from Transformers) is a powerful tool for question answering tasks due to its ability to understand contextual information in input text. This project focuses on fine-tuning a BERT model for question answering using a limited dataset for illustration purposes.

## Steps Involved in Fine-tuning
1. **Data Preparation:**
   
2. **Define Questions and Answers:**

3. **Data Format Conversion:**

4. **Setting up Testing Data:**

5. **Training for Fine-tuning:**
   - Installed SimpleTransformers for BERT model fine-tuning.
   - Used the 340M parameter bert-large-uncased BERT model with 50 epochs.

6. **Model Evaluation:**

7. **Model Inference:**

<!-- Sources -->
## üåê Sources
1. [Hugging Face - Wikipedia](https://en.wikipedia.org/wiki/Hugging_Face)
2. [Build a Smart Question Answering System with Fine-Tuned BERT](https://medium.com/saarthi-ai/build-a-smart-question-answering-system-with-fine-tuned-bert-b586e4cfa5f5)
3. [Fine-Tune Transformer Models For Question Answering On Custom Data](https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80)
</xml>
